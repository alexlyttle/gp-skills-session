{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Processes\n",
    "\n",
    "**Alex Lyttle | 19 November 2021**\n",
    "\n",
    "This skills session will introduce you to Gaussian processes from scratch, finishing with a simple astrophysical application.\n",
    "\n",
    "#### Install packages\n",
    "\n",
    "Run this cell if using Google Colab, otherwise take care when installing packages in your own environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn, emcee, corner, version_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext version_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import emcee\n",
    "\n",
    "from numpy import random\n",
    "from scipy import stats, optimize\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from corner import corner\n",
    "\n",
    "%version_information numpy, scipy, matplotlib, seaborn, emcee, corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='ticks', palette='colorblind', font_scale=1.2)  # Set theme\n",
    "plt.set_cmap('Blues')  # Set default color map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal distribution\n",
    "\n",
    "If some random variable $y$ is normally distributed, its probability density function $p(y) = \\mathcal{N}(\\mu, \\sigma^2)$ where,\n",
    "\n",
    "$$\\mathcal{N}(\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left[-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right],$$\n",
    "\n",
    "$\\mu$ is the mean, and $\\sigma^2$ is the variance ($\\sigma$ is the standard deviation). We may paraphrase this by writing that a given $y$ is drawn from a normal distribution like this,\n",
    "\n",
    "$$y \\sim \\mathcal{N}(\\mu, \\sigma^2).$$\n",
    "\n",
    "We can draw (pseudo) random numbers from this distribution with `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal(y, loc=0.0, scale=1.0):\n",
    "    \"\"\"TODO: return the normal distribution. \n",
    "    \n",
    "    Hint:\n",
    "        Use np.exp and np.pi.\n",
    "    \"\"\"\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = ...  # TODO: choose a random seed\n",
    "rng = random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0.0\n",
    "sigma = 1.0\n",
    "\n",
    "y = np.linspace(-4, 4, 101)\n",
    "density = normal(y, loc=mean, scale=sigma)\n",
    "\n",
    "num_samples = 100\n",
    "rvs = rng.normal(loc=mean, scale=sigma, size=num_samples)\n",
    "\n",
    "plt.plot(y, density, label='pdf')\n",
    "plt.hist(rvs, bins=10, density=True, alpha=0.5, label='samples')\n",
    "plt.xlabel('y')\n",
    "plt.ylabel('probability density')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate normal distribution\n",
    "\n",
    "The multivariate normal distribution is the general case for the above for $n$-dimensional variable $\\boldsymbol{y} = y_0, y_1, \\dots, y_{n-1}$.\n",
    "\n",
    "$$\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{\\sqrt{(2\\pi)^{n}|\\boldsymbol{\\Sigma}|}}\\exp\\left[-\\frac12 (\\boldsymbol{y} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{y} - \\boldsymbol{\\mu})\\right]$$\n",
    "\n",
    "$$p(\\boldsymbol{y}) = \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$$\n",
    "\n",
    "This is a little more complicated to code up, so we use the `scipy.stats` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mv_normal(y, mean, cov):\n",
    "    \"\"\"N-dimensional multivariate normal probability density function\n",
    "    p(y) = N(mean, cov).\n",
    "    \n",
    "    Args:\n",
    "        y (array_like): Array of coordinates to evaluate the pdf with length N.\n",
    "        mean (array_like): The mean of the MV Normal (1-D array of size N)\n",
    "        cov (array_like): The covariance of the MV Normal (2-D array of shape (N, N)).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: N-D array of probability density.\n",
    "    \"\"\"\n",
    "    Y = np.meshgrid(*y)\n",
    "    x = np.stack(Y, axis=-1)\n",
    "    \n",
    "    density = stats.multivariate_normal.pdf(x, mean=mean, cov=cov)\n",
    "    return density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint distribution\n",
    "\n",
    "Let's consider a 2-D multivariate normal, $\\boldsymbol{y} = [y_0, y_1]$, variables $y_0$ and $y_1$ go as,\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    y_0 \\\\ y_1\n",
    "\\end{bmatrix}\n",
    "\\sim \\mathcal{N} \\left(\n",
    "\\begin{bmatrix}\n",
    "    \\mu_0 \\\\ \\mu_1\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "    \\Sigma_{0,0} & \\Sigma_{0,1} \\\\\n",
    "    \\Sigma_{1,0} & \\Sigma_{1,1}\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "where the mean,\n",
    "\n",
    "$$\\mu_i = \\mathbb{E}[y_i]$$\n",
    "\n",
    "and the covariance,\n",
    "\n",
    "$$\\Sigma_{i,j} \\equiv \\mathrm{cov}(y_i, y_j) = \\mathbb{E}[(y_i - \\mu_i)\\cdot(y_j - \\mu_j)]$$\n",
    "\n",
    "The variance is just a special case of the covariance for $i=j$,\n",
    "\n",
    "$$\\sigma_i^2 \\equiv \\mathrm{var}(y_i) = \\mathbb{E}[(y_i - \\mu_i)^2]$$\n",
    "\n",
    "For an $N$-dimensional multivariate normal in $\\boldsymbol{y}$, the joint (or 2-D marginal) distribution is integrated over all $\\boldsymbol{y}_{k} = \\{y_k\\}$ where $k \\neq i,j$,\n",
    "\n",
    "$$p(y_i, y_j) = \\int_{\\boldsymbol{y}_{k}} p(\\boldsymbol{y}) \\, \\mathrm{d}\\boldsymbol{y}_{k}.$$\n",
    "\n",
    "If we evaluate this integral we get the 2-D multivariate normal,\n",
    "\n",
    "$$p(y_i, y_j) = \\mathcal{N} \\left(\n",
    "\\begin{bmatrix}\n",
    "    \\mu_i \\\\ \\mu_j\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "    \\Sigma_{i,i} & \\Sigma_{i,j} \\\\\n",
    "    \\Sigma_{j,i} & \\Sigma_{j,j}\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_joint(y, mean, cov, i=0, j=1):\n",
    "    \"\"\"TODO: Get joint distribution p(y_i, y_j).\n",
    "    \n",
    "    Args:\n",
    "        y (array_like): Array of coordinates to evaluate the pdf with length N.\n",
    "        mean (array_like): The mean of y (1-D array of size N)\n",
    "        cov (array_like): The covariance of y (2-D array of shape (N, N)).\n",
    "        i, j (int): Indices in joint distribution.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: 2-D array of joint probability density.\n",
    "    \"\"\"\n",
    "    mean = np.array(mean)\n",
    "    joint_mean = ...\n",
    "\n",
    "    cov = np.array(cov)\n",
    "    joint_cov = [\n",
    "        [..., ...],\n",
    "        [..., ...]\n",
    "    ]\n",
    "    return mv_normal([y[i], y[j]], joint_mean, joint_cov)    \n",
    "\n",
    "\n",
    "def plot_joint(y, mean, cov, i=0, j=1, ax=None, cax=None):\n",
    "    \"\"\"Plots the joint distribution, p(y_i, y_j).\n",
    "\n",
    "    Args:\n",
    "        y (array_like): Array of coordinates to evaluate the pdf with length N.\n",
    "        mean (array_like): The mean of y (1-D array of size N)\n",
    "        cov (array_like): The covariance of y (2-D array of shape (N, N)).\n",
    "        i, j (int): Indices in joint distribution.\n",
    "        vertical (bool): Display distribution vertically.\n",
    "        ax (matplotlib.axes.Axes, optional): Axes object to make plot.\n",
    "    \n",
    "    Returns:\n",
    "        matplotlib.axes.Axes: Plot axis.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    assert i != j\n",
    "\n",
    "    joint = get_joint(y, mean, cov, i=i, j=j)\n",
    "    \n",
    "    c = ax.contourf(y[i], y[j], joint)\n",
    "    ax.set_xlabel(rf'$y_{i}$')\n",
    "    ax.set_ylabel(rf'$y_{j}$')\n",
    "\n",
    "    cbar = plt.colorbar(c, cax=cax, pad=0.075)\n",
    "    cbar.set_label('probability density')\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "def grid(mean, cov, num_points=201, width_factor=3.0):\n",
    "    \"\"\"A grid of points y with shape (N, M) goverened by a window\n",
    "    around the mean with width determined by the covariance.\n",
    "    \n",
    "    Args:\n",
    "        mean (array_like): The mean of y (1-D array of size N)\n",
    "        cov (array_like): The covariance of y (2-D array of shape (N, N)).\n",
    "        num_points (int): The number of points in an axis of the grid (M).\n",
    "        width_factor (float): The factor to multiply sigma for the width of\n",
    "            each axis.\n",
    "    \n",
    "    Returns:\n",
    "        y (np.ndarray): A N x M grid of points.\n",
    "    \"\"\"\n",
    "    sigma = np.sqrt(np.diag(cov))\n",
    "    y = np.linspace(mean-width_factor*sigma, mean+width_factor*sigma, num_points, axis=1)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.zeros(2)\n",
    "cov = np.array([[..., ...],  # TODO: construct a covariance matrix\n",
    "                [..., ...]])\n",
    "\n",
    "y = ...  # TODO: use above grid function to get y\n",
    "ax = plot_joint(y, mean, cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal distribution\n",
    "\n",
    "A marginal distribution is just one in which parameters in the full distribution have been integrated out (as we saw with the joint distribution). If our full distribution is $p(y_0, y_1)$ then the marginal distribution for $y_0$ is,\n",
    "\n",
    "$$p(y_0) = \\int p(y_0, y_1) \\, \\mathrm{d}y_1.$$\n",
    "\n",
    "If we evaluate this integral we get the normal distribution,\n",
    "\n",
    "$$y_0 \\sim \\mathcal{N}(\\mu_0, \\sigma_{0}^2)$$\n",
    "\n",
    "where $\\sigma_{0}^2 \\equiv \\Sigma_{0, 0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_marginal(y, mean, cov, j=0):\n",
    "    \"\"\"TODO: Gets the marginal distribution p(y_j).\n",
    "    \n",
    "    Args:\n",
    "        y (array_like): Array of coordinates to evaluate the pdf with length N.\n",
    "        mean (array_like): The mean of y (1-D array of size N)\n",
    "        cov (array_like): The covariance of y (2-D array of shape (N, N)).\n",
    "        j (int): Index to marginalise out.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: 1-D array of marginal probability density.\n",
    "    \"\"\"\n",
    "    loc = ...\n",
    "    cov = np.array(cov)\n",
    "    scale = ...\n",
    "    return normal(y[j], loc=loc, scale=scale)\n",
    "    \n",
    "    \n",
    "def plot_marginal(y, mean, cov, j=0, vertical=True, ax=None):\n",
    "    \"\"\"Plots the marginal distribution p(y_j).\n",
    "    \n",
    "    Args:\n",
    "        y (array_like): Array of coordinates to evaluate the pdf with length N.\n",
    "        mean (array_like): The mean of y (1-D array of size N)\n",
    "        cov (array_like): The covariance of y (2-D array of shape (N, N)).\n",
    "        j (int): Index to marginalise out.\n",
    "        vertical (bool): Display distribution vertically.\n",
    "        ax (matplotlib.axes.Axes, optional): Axes object to make plot.\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib.axes.Axes: Plot axis.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    marginal = get_marginal(y, mean, cov, j=j)\n",
    "    \n",
    "    X, Y = (y[j], marginal)\n",
    "    xaxis, yaxis = (ax.xaxis, ax.yaxis)\n",
    "    if vertical:\n",
    "        X, Y = Y, X\n",
    "        xaxis, yaxis = yaxis, xaxis\n",
    "\n",
    "    ax.plot(X, Y, label=rf'$p(y_{j})$')\n",
    "    \n",
    "    xaxis.set_label_text(rf'$y_{j}$')\n",
    "    yaxis.set_label_text('probability density')\n",
    "    \n",
    "    return ax\n",
    "\n",
    "def plot_joint_marginal(y, mean, cov, i=0, j=1, fig=None):\n",
    "    \"\"\"Plots the joint distribution p(y_i, y_j) and the\n",
    "    marginal distribution p(y_j) in the same figure.\n",
    "    \n",
    "    Args:\n",
    "        y (array_like): Array of coordinates to evaluate the pdf with length N.\n",
    "        mean (array_like): The mean of y (1-D array of size N)\n",
    "        cov (array_like): The covariance of y (2-D array of shape (N, N)).\n",
    "        i, j (int): Indices in joint distribution.\n",
    "        vertical (bool): Display distribution vertically.\n",
    "        fig (matplotlib.figure.Figure, optional): Figure object to make plot.\n",
    "    \n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: Figure object.\n",
    "    \"\"\"\n",
    "    if fig is None:\n",
    "        width_ratios = [1.0, 0.625, 0.075]\n",
    "        sf = 5  # scale factor for plot size\n",
    "        figsize = sf * np.array([sum(width_ratios), width_ratios[0]])\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "\n",
    "    gs = fig.add_gridspec(1, 3, wspace=0.1, width_ratios=width_ratios)\n",
    "\n",
    "    ax0 = fig.add_subplot(gs[0])\n",
    "    cax = fig.add_subplot(gs[2])\n",
    "    ax0 = plot_joint(y, mean, cov, i=i, j=j, ax=ax0, cax=cax)\n",
    "\n",
    "    ax1 = fig.add_subplot(gs[1], sharey=ax0)\n",
    "    ax1 = plot_marginal(y, mean, cov, j=j, ax=ax1)\n",
    "    ax1.tick_params(labelleft=False)\n",
    "    ax1.yaxis.label.set_visible(False)\n",
    "\n",
    "    return fig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_joint_marginal(y, mean, cov)\n",
    "fig.legend(loc='upper center');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional distribution\n",
    "\n",
    "The conditional distribution of $y_1$ given some value of $y_0$ is given by the joint distribution over the marginal distribution,\n",
    "\n",
    "$$p(y_1 | y_0) = \\frac{p(y_0, y_1)}{p(y_0)}.$$\n",
    "\n",
    "We will later show what this evaluates to, but for now we will just numerically calculate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conditional(y, mean, cov, i=0, j=1, fig=None):\n",
    "    \"\"\"Plot the conditional distribution of y_j given y_i, \n",
    "    p(y_j | y_i).\n",
    "    \n",
    "    Args:\n",
    "        y (list of array_like): Coordinates of the MV normal\n",
    "            where y[i] must be a float and all other indeces\n",
    "            must be array_like.\n",
    "        mean (array_like): The mean of y (1-D array of size N)\n",
    "        cov (array_like): The covariance of y (2-D array of shape (N, N)).\n",
    "        i (int): Index of the condition.\n",
    "        j (int): Index of the conditional variable.\n",
    "        fig (matplotlib.figure.Figure, optional): Figure object to make plot.\n",
    "    \n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: Figure object.    \n",
    "    \"\"\"\n",
    "    if fig is None:\n",
    "        fig = plt.gcf()\n",
    "    \n",
    "    joint = get_joint(y, mean, cov, i=i, j=j)\n",
    "    marginal = get_marginal(y, mean, cov, j=i)\n",
    "\n",
    "    cond = joint/marginal\n",
    "\n",
    "    axes = fig.get_axes()\n",
    "    axes[0].axvline(y[i], color='C1', label=rf'$y_{i} = {y[i]:.1f}$')\n",
    "    axes[2].plot(cond, y[j], color='C1', linestyle='--', label=rf'$p(y_{j} | y_{i} = {y[i]:.1f})$')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_cond = list(y)\n",
    "y_cond[0] = ...  # TODO: choose condition on y_0\n",
    "\n",
    "fig = plot_joint_marginal(y, mean, cov)\n",
    "fig = plot_conditional(y_cond, mean, cov, fig=fig)\n",
    "fig.legend(loc='upper center', ncol=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random variables\n",
    "\n",
    "Now let's draw random variables from the distribution. We do this with the `numpy.random` module.\n",
    "\n",
    "Firstly, we define some helper functions for plotting the samples and relationship between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(samples, i=0, j=1, ax=None):\n",
    "    \"\"\"Plot the samples[i] against samples[j].   \n",
    "\n",
    "    Args:\n",
    "        samples (array_like): Array of samples.\n",
    "        i, j (int): Indices of samples to plot.\n",
    "        ax (matplotlib.axes.Axes): Axis on which to plot.\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib.axes.Axes: Plotting axis.\n",
    "    \"\"\"\n",
    "    alpha = 1/samples.shape[0]**0.1\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.plot(*samples[:, [i, j]].T, color='C1', marker='o', linestyle='none', alpha=alpha, label='samples')\n",
    "    return ax\n",
    "\n",
    "def plot_relation(samples, ax=None):\n",
    "    \"\"\"Plot the relationship between each index of samples.\n",
    "    \n",
    "    Args:\n",
    "        samples (array_like): Array of samples.\n",
    "        ax (matplotlib.axes.Axes): Axis on which to plot.\n",
    "        \n",
    "    Returns:\n",
    "        matplotlib.axes.Axes: Plotting axis.\n",
    "    \"\"\"\n",
    "    alpha = 1/samples.shape[0]**0.1\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.plot(samples.T, color='C1', marker='o', alpha=alpha)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.set_xlabel('index')\n",
    "    ax.set_ylabel('$y_i$')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlated distribution\n",
    "\n",
    "We see that the choice of $y_0$ correlates with that of $y_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = ...  # TODO: choose number of samples to plot\n",
    "samples = rng.multivariate_normal(mean, cov, size=num_samples)\n",
    "\n",
    "figsize = plt.rcParamsDefault['figure.figsize'].copy()\n",
    "figsize[0] *= 2\n",
    "fig = plt.figure(figsize=figsize, tight_layout=True)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax = plot_joint(y, mean, cov, ax=ax)\n",
    "ax = plot_samples(samples, ax=ax)\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax = plot_relation(samples, ax=ax)\n",
    "ax.set_xlim(-0.5, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncorrelated distribution\n",
    "\n",
    "For the uncorrelated case, the conditional distribution is the same everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.zeros(2)\n",
    "cov = [[..., ...],  # TODO: define uncorrelated 2 x 2 covariance matrix\n",
    "       [..., ...]]\n",
    "y = grid(mean, cov)\n",
    "\n",
    "y_cond = list(y)\n",
    "y_cond[0] = ...  # TODO: choose condition on y_0\n",
    "\n",
    "fig = plot_joint_marginal(y, mean, cov)\n",
    "fig = plot_conditional(y_cond, mean, cov, fig=fig)\n",
    "fig.legend(loc='upper center', ncol=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this when we take random samples. There is no correlation between $y_0$ and $y_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = ...  # TODO: choose number of samples\n",
    "samples = rng.multivariate_normal(mean, cov, size=num_samples)\n",
    "\n",
    "figsize = plt.rcParamsDefault['figure.figsize'].copy()\n",
    "figsize[0] *= 2\n",
    "fig = plt.figure(figsize=figsize, tight_layout=True)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax = plot_joint(y, mean, cov, ax=ax)\n",
    "ax = plot_samples(samples, ax=ax)\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax = plot_relation(samples, ax=ax)\n",
    "ax.set_xlim(-0.5, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding dimensions\n",
    "\n",
    "Now lets add some dimensions to the distribution. We need a helper function which gives us a valid covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(index):\n",
    "    \"\"\"Get a covariance matrix for a given array of indices.\n",
    "\n",
    "    Args:\n",
    "        index (array_like): Array of indeces. \n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Covariance matrix.\n",
    "    \"\"\"\n",
    "    index = np.array(index)\n",
    "    return np.exp(-0.5 * (index[:, None] - index)**2/index.size)\n",
    "\n",
    "\n",
    "def plot_covariance(cov, ax=None):\n",
    "    \"\"\"Plot the covariance matrix.\n",
    "    \n",
    "    Args:\n",
    "        cov (array_like): Covariance matrix.\n",
    "        ax (matplotlib.axes.Axes, optional). Axis on which to plot.\n",
    "    \n",
    "    Returns:\n",
    "        matplotlib.axes.Axes: Plotting axis.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    matrix = ax.matshow(cov)\n",
    "    ax.set_ylabel('index')\n",
    "    ax.set_xlabel('index')\n",
    "    ax.xaxis.set_label_position('top') \n",
    "\n",
    "    cbar = plt.colorbar(matrix)\n",
    "    cbar.set_label('covariance')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dims = 3\n",
    "index = np.arange(num_dims)\n",
    "mean = np.zeros(num_dims)\n",
    "cov = covariance(index)  # Creates an example covarience matrix for given index.\n",
    "\n",
    "num_samples = ...  # TODO: choose number of samples\n",
    "samples = rng.multivariate_normal(mean, cov, size=num_samples)\n",
    "\n",
    "fig = plt.figure(figsize=figsize, tight_layout=True)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax = plot_covariance(cov, ax=ax);\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax = plot_relation(samples, ax=ax)\n",
    "ax.set_xlim(-0.5, num_dims-0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try even more dimensions! The relationship plot is begining to look like a smooth function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dims = ...  # TODO: choose number of dimensions\n",
    "index = np.arange(num_dims)\n",
    "mean = np.zeros(num_dims)\n",
    "cov = covariance(index)\n",
    "\n",
    "num_samples = ...  # TODO: choose number of samples\n",
    "samples = rng.multivariate_normal(mean, cov, size=num_samples)\n",
    "\n",
    "fig = plt.figure(figsize=figsize, tight_layout=True)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax = plot_covariance(cov, ax=ax);\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax = plot_relation(samples, ax=ax)\n",
    "ax.set_xlim(-0.5, num_dims-0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the joint distribution. Change `i` and `j` to see how the distributions vary between indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = grid(mean, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, j = (...)  # TODO: choose i and j\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax = plot_joint(y, mean, cov, i=i, j=j, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian process\n",
    "\n",
    "The Gaussian process condition is that some function $f(x)$ is correlated with it's value at $x'$ according to a mean function, $m$ and kernel function $k$,\n",
    "\n",
    "$$f(\\boldsymbol{x}) \\sim \\mathcal{GP}[m(\\boldsymbol{x}), k(\\boldsymbol{x}, \\boldsymbol{x}')].$$\n",
    "\n",
    "If we make measurements $\\boldsymbol{y}$ of the function at an array of points $\\boldsymbol{x}$, then to satisfy the GP condition we say the points $\\boldsymbol{y}$ must be drawn from the multivariate normal distribution,\n",
    "\n",
    "$$\\boldsymbol{y} \\sim \\mathcal{N}(\\boldsymbol{m}, \\boldsymbol{K}).$$\n",
    "\n",
    "where $\\boldsymbol{m} = m(\\boldsymbol{x})$ and $\\boldsymbol{K} = k(\\boldsymbol{x}, \\boldsymbol{x})$.\n",
    "\n",
    "$$\\boldsymbol{K} = \n",
    "\\begin{bmatrix}\n",
    "    K_{00} & K_{01} & \\dots \\\\\n",
    "    K_{10} & \\dots & \\dots \\\\\n",
    "    \\dots & \\dots & \\dots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The mean function represents the behaviour of the function we would expect if there was no correlated noise. The kernel function represents the covarience (or correlation) between the value of the function at different values of $x$. There are many kernel functions to choose from, but we will start with a simple example called the Squared Exponential Kernel,\n",
    "\n",
    "$$K_{ij} = \\sigma^2 \\exp\\left[- \\frac{(x_i - x_j)^2}{2\\lambda^2}\\right],$$\n",
    "\n",
    "where $\\sigma$ and $\\lambda$ are the kernel scale and lengthscale respectively.\n",
    "\n",
    "### Predictions\n",
    "\n",
    "To satisfy the GP condition, the value of the function, $\\boldsymbol{y}_\\star$ evaluated at $\\boldsymbol{x}_\\star$ are correlated with $\\boldsymbol{y}$, and are a part of the joint distribution,\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\boldsymbol{y} \\\\ \\boldsymbol{y}_\\star\n",
    "\\end{bmatrix}\n",
    "\\sim \\mathcal{N} \\left(\n",
    "\\begin{bmatrix}\n",
    "    \\boldsymbol{m} \\\\ \\boldsymbol{m}_\\star\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "    \\boldsymbol{K} & \\boldsymbol{K}_\\star \\\\\n",
    "    \\boldsymbol{K}_\\star^T & \\boldsymbol{K}_{\\star\\star}\n",
    "\\end{bmatrix}\n",
    "\\right),\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{K}_\\star = k(\\boldsymbol{x}, \\boldsymbol{x}_\\star)$ and $\\boldsymbol{K}_{\\star\\star} = k(\\boldsymbol{x}_\\star, \\boldsymbol{x}_\\star)$. Using the conditional distribution equation, $p(y_\\star | y) = p(y, y_\\star) / p(y)$, it can be derived that,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{y}_\\star | \\boldsymbol{y} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_\\star, \\boldsymbol{\\Sigma}_\\star)\n",
    "$$\n",
    "\n",
    "where,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\mu}_\\star &= \\boldsymbol{m}_\\star + \\boldsymbol{K}_\\star \\cdot \\boldsymbol{K}^{-1} \\cdot (\\boldsymbol{y} - \\boldsymbol{m}) \\\\\n",
    "\\boldsymbol{\\Sigma}_\\star &= \\boldsymbol{K}_{\\star\\star} - \\boldsymbol{K}_\\star \\cdot \\boldsymbol{K}^{-1} \\cdot \\boldsymbol{K}_\\star^T\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jitter(x, delta=1e-6):\n",
    "    \"\"\"A small amount to add to the diagonal of the covariance.\"\"\"\n",
    "    return delta * np.eye(x.shape[0])\n",
    "\n",
    "\n",
    "def squared_exponential(x, xp, sigma=1.0, length=1.0):\n",
    "    \"\"\"The squared exponential kernel function.\n",
    "    \n",
    "    Args:\n",
    "        x (array_like): Array of x.\n",
    "        xp (array_like): Array of x'.\n",
    "        sigma (float): The kernel amplitude.\n",
    "        length (float): The kernel length scale.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: 2-D covariance matrix.\n",
    "    \"\"\"\n",
    "    return sigma**2 * np.exp(- 0.5 * (x[:, np.newaxis] - xp)**2 / length**2)\n",
    "\n",
    "\n",
    "def predict(x_pred, x, y, *, mean, kernel):\n",
    "    \"\"\"TODO: Make a prediction from the GP governed by a given kernel.\n",
    "    \n",
    "    Args:\n",
    "        x_pred (array_like): Array of x-values at which to make predictions.\n",
    "        x (array_like): Array of x-values for known y-values.\n",
    "        y (array_like): Array of known y-values.\n",
    "        mean (callable): Mean function.\n",
    "        kernel (callable): Kernel function.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: 1-D prediction mean.\n",
    "        np.ndarray: 2-D prediction covariance.\n",
    "\n",
    "    Hints:\n",
    "        1. Firstly, compute the three kernels K.\n",
    "        2. Inverting a matrix can be done with np.linalg.inv().\n",
    "        2. Use np.dot() to compute the dot product.\n",
    "        3. Add jitter(x) and jitter(x_pred) to k(x, x) and k(x_pred, x_pred)\n",
    "            to ensure that it is positive semidefinite. \n",
    "    \"\"\"\n",
    "    Kxx = ...\n",
    "    Kxp = ...\n",
    "    Kpp = ...\n",
    "    Kxx_inv = ...  # Inversion of Kxx\n",
    "        \n",
    "    mu_pred = ...\n",
    "    cov_pred = ...\n",
    "    \n",
    "    return mu_pred, cov_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep our life simple, we will consider the case where the mean function $m(\\boldsymbol{x}) = \\boldsymbol{0}$ (using `np.zeros_like`).\n",
    "\n",
    "In the folowing cell, define a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs = 10\n",
    "x_low, x_high = (0, 10)\n",
    "x = rng.uniform(x_low, x_high, size=num_obs)  # Observed x\n",
    "\n",
    "num_pred = 51\n",
    "x_pred = np.linspace(x_low, x_high, num_pred)  # Unobserved x\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"TODO: Define a true smooth function f.\n",
    "    \n",
    "    Args:\n",
    "        x (array_like): Input array.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Output array.\n",
    "    \n",
    "    Hint:\n",
    "        np.sin or np.cos are good examples.\n",
    "    \"\"\"\n",
    "    return ...\n",
    "\n",
    "y = f(x)  # Observed values of f at x\n",
    "y_true = f(x_pred)  # True values of f at x_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(x, y, x_pred, mu_pred, sigma_pred, y_err=0.0):\n",
    "    plt.errorbar(x, y, yerr=y_err, fmt='o')\n",
    "    plt.plot(x_pred, mu_pred, label=r'$\\mu$')\n",
    "    plt.fill_between(x_pred, mu_pred-sigma_pred, mu_pred+sigma_pred, alpha=0.33, color='C1', label=r'$2\\sigma$')\n",
    "\n",
    "    plt.plot(x_pred, y_true, '--', label='truth')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: choose sigma and length to best fit the data\n",
    "sigma = ...\n",
    "length = ...\n",
    "\n",
    "mean = np.zeros_like\n",
    "kernel = lambda x, xp: squared_exponential(x, xp, sigma=length, length=length)\n",
    "\n",
    "mu_pred, cov_pred = predict(x_pred, x, y, mean=mean, kernel=kernel)\n",
    "sigma_pred = np.sqrt(np.diag(cov_pred))\n",
    "\n",
    "plot_predictions(x, y, x_pred, mu_pred, sigma_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observational noise\n",
    "\n",
    "What if our observations of $f(x)$ have some additional independent white noise?\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\boldsymbol{y} \\\\ \\boldsymbol{y}_\\star\n",
    "\\end{bmatrix}\n",
    "\\sim \\mathcal{N} \\left(\n",
    "\\begin{bmatrix}\n",
    "    \\boldsymbol{m} \\\\ \\boldsymbol{m}_\\star\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "    \\boldsymbol{K} + \\sigma_y^2\\mathcal{I} & \\boldsymbol{K}_\\star \\\\\n",
    "    \\boldsymbol{K}_\\star^T & \\boldsymbol{K}_{\\star\\star}\n",
    "\\end{bmatrix}\n",
    "\\right),\n",
    "$$\n",
    "\n",
    "where $\\mathcal{I}$ is the identity matrix.\n",
    "\n",
    "We need to update `predict` to account for this noise. Using your solution from before, add `white_noise` to $\\boldsymbol{K}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def white_noise(x, scale=1.0):\n",
    "    \"\"\"White noise to add to the diagonal of the covariance.\"\"\"\n",
    "    return scale**2 * np.eye(x.shape[0])\n",
    "\n",
    "\n",
    "def predict(x_pred, x, y, *, mean, kernel, y_err=0.0):\n",
    "    \"\"\"TODO: Make a prediction from the GP governed by a given kernel.\n",
    "    \n",
    "    Args:\n",
    "        x_pred (array_like): Array of x-values at which to make predictions.\n",
    "        x (array_like): Array of x-values for known y-values.\n",
    "        y (array_like): Array of known y-values.\n",
    "        mean (callable): Mean function.\n",
    "        kernel (callable): Kernel function.\n",
    "        y_err (float): Observed uncertainty on y.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 1-D prediction mean.\n",
    "        np.ndarray: 2-D prediction covariance.\n",
    "\n",
    "    Hints:\n",
    "        1. The scale of the white noise is given by y_err.\n",
    "        2. We only want to add white noise to k(x, x).\n",
    "    \"\"\"\n",
    "    Kxx = ...\n",
    "    Kxp = ...\n",
    "    Kpp = ...\n",
    "    Kxx_inv = ...\n",
    "        \n",
    "    mu_pred = ...\n",
    "    cov_pred = ...\n",
    "    \n",
    "    return mu_pred, cov_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_err = 0.1\n",
    "y = f(x) + y_err * rng.normal(size=num_obs)  # Adding obs noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: choose sigma and length to best fit the data\n",
    "sigma = ...\n",
    "length = ...\n",
    "\n",
    "mean = np.zeros_like\n",
    "kernel = lambda x, xp: squared_exponential(x, xp, sigma=length, length=length)\n",
    "\n",
    "mu_pred, cov_pred = predict(x_pred, x, y, y_err=y_err, \n",
    "                            mean=mean, kernel=kernel)\n",
    "sigma_pred = np.sqrt(np.diag(cov_pred))\n",
    "\n",
    "plot_predictions(x, y, x_pred, mu_pred, sigma_pred, y_err=y_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial velocities example\n",
    "\n",
    "Let's consider the simplified radial velocity a star with a single orbiting planet,\n",
    "\n",
    "$$f_r(t, A, P) = A \\sin\\left(\\frac{2\\pi}{P}t\\right)$$\n",
    "\n",
    "where $A$ and $P$ are the amplitude and period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radial_velocity(t, amplitude=1.0, period=1.0):\n",
    "    return amplitude * np.cos(2 * np.pi * t / period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Lets consider the case where $A = 5$ (arbitrary units) and $P = 8\\,\\mathrm{d}$.\n",
    "\n",
    "We observe 51 times over 24 days. However, there is some systematic noise messing with our measurements. We do not know the source of this noise, it could come from other orbiting bodies, activity of the star, or instrumentation issues.\n",
    "\n",
    "Here, we inject the correlated noise using a sum of sine waves, but we will pretend we do not know its functional form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make up true values. Keep them between of order 1 to 10\n",
    "truth = {\n",
    "    'amplitude': ...,  # arbitrary units\n",
    "    'period': ...,  # days\n",
    "}\n",
    "\n",
    "num_obs = ...  # TODO: how many observations do we make?\n",
    "num_pred = ...  # TODO: how many predictions (> num_obs)\n",
    "duration = ...  # TODO: what is the duration of observations (should be >~ period)\n",
    "\n",
    "t = np.linspace(0, duration, num_obs)\n",
    "t_pred = np.linspace(0, duration, num_pred)\n",
    "\n",
    "# Contribution from the planet\n",
    "planet = radial_velocity(t, **truth)\n",
    "planet_true = radial_velocity(t_pred, **truth)\n",
    "\n",
    "# Injected correlated noise\n",
    "w = 0.3  # Arbitrary scale of the noise\n",
    "delta = lambda t: np.sin(w*t) + np.sin(w*t)**2 + np.sin(w*t)**3\n",
    "vr = planet + delta(t)\n",
    "vr_true = planet_true + delta(t_pred)\n",
    "\n",
    "# Observed uncertainty on the velocities.\n",
    "vr_err = 0.5\n",
    "vr += vr_err * rng.normal(size=num_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the observation against the truths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.errorbar(t, vr, yerr=vr_err, fmt='o', label='obs')\n",
    "plt.plot(t_pred, planet_true, linestyle='--', label='planet');\n",
    "\n",
    "plt.plot(t_pred, vr_true, linestyle='--', label='truth');\n",
    "plt.legend()\n",
    "plt.xlabel('time (days)')\n",
    "plt.ylabel('radial velocity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple model\n",
    "\n",
    "Using `emcee`, we will attempt to model this data with Bayes' theorem for the posterior probability of the model given the data,\n",
    "\n",
    "$$p(\\theta|v_r) \\propto \\mathcal{L}(v_r|\\theta)\\,\\Pi(\\theta)$$\n",
    "\n",
    "where $\\mathcal{L}(v_r|\\theta)$ is the likelihood of the data, $v_r$ given the model parameters $\\theta$, and $\\Pi(\\theta)$ is the prior likelihood of the model parameters.\n",
    "\n",
    "The `log_prior` function defined below describes out prior expectation of the amplitude and period of the signal. The `log_likelihood` assumes known independent Gaussian noise characterised by $\\sigma_v$,\n",
    "\n",
    "$$\\ln\\mathcal{L}(v_r|\\theta) = - \\frac12 \\sum_i \\left\\{ \\left[\\frac{v_{r, i} - f_{r}(t_i, \\theta)}{\\sigma_v}\\right]^2 + 2\\ln(\\sigma_v) + \\log(2\\pi) \\right\\}$$\n",
    "\n",
    "For this simple model, our parameters $\\theta = \\{A, P\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(params):\n",
    "    \"\"\"TODO: Give sensible priors for a and p.\"\"\"\n",
    "    a, p = params  # amplitude, period\n",
    "    \n",
    "    if ... < a < ... and ... < p < ...:\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "\n",
    "def log_likelihood(params, t, vr, vr_err):\n",
    "    a, p = params  # amplitude, period\n",
    "\n",
    "    mu = radial_velocity(t, amplitude=a, period=p)\n",
    "    return - 0.5 * np.sum(((vr - mu) / vr_err)**2 + 2*np.log(vr_err) + np.log(2*np.pi))\n",
    "\n",
    "\n",
    "def log_prob(params, t, vr, vr_err):\n",
    "    lprior = log_prior(params)\n",
    "    if not np.isfinite(lprior):\n",
    "        return -np.inf\n",
    "    return lprior + log_likelihood(params, t, vr, vr_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We setup our sampler for this 2-dimensional model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dim, num_walkers = 2, 100\n",
    "sampler = emcee.EnsembleSampler(num_walkers, num_dim, log_prob, args=(t, vr, vr_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose some initial parameters in the bounds of our prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = [..., ...]  # TODO: choose initial param values\n",
    "shape = (num_walkers, num_dim)\n",
    "\n",
    "# Broadcast parameters to sampler shape and add some random jitter\n",
    "init_params = np.broadcast_to(init_params, shape) + 1e-6 * rng.normal(size=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run MCMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running burn-in...\")\n",
    "\n",
    "state, lp, _ = sampler.run_mcmc(init_params, 1000)\n",
    "\n",
    "print(\"Running production...\")\n",
    "sampler.reset()\n",
    "sampler.run_mcmc(state, 1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot using the `corner` package, we see that the parameter estimations are biased and inaccurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.flatchain\n",
    "labels = ['amplitude', 'period']\n",
    "truths = list(truth.values())\n",
    "corner(samples, show_titles=True, labels=labels, truths=truths);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions\n",
    "\n",
    "Let's plot the predictions against the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_pred = radial_velocity(t_pred, amplitude=samples[:, 0, None], period=samples[:, 1, None])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(t, vr, yerr=vr_err, fmt='o', label='obs')\n",
    "\n",
    "vr_mean = vr_pred.mean(axis=0)\n",
    "vr_sd = vr_pred.std(axis=0)\n",
    "plt.plot(t_pred, vr_mean, label=r'$\\mu$')\n",
    "plt.fill_between(t_pred, vr_mean-vr_sd, vr_mean+vr_sd, alpha=0.33, color='C1', label=r'$2\\sigma$')\n",
    "\n",
    "plt.plot(t_pred, vr_true, linestyle='--', label='truth');\n",
    "plt.legend()\n",
    "plt.xlabel('time (days)')\n",
    "plt.ylabel('radial velocity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GP model\n",
    "\n",
    "Now let's model using a GP. We will add the GP kernel parameters $\\sigma$ and $\\lambda$ to our model and choose a sensible prior for them. Our model parameters $\\theta = \\{A, P, \\sigma, \\lambda\\}$.\n",
    "\n",
    "Now, our GP likelihood is a little more complicated,\n",
    "\n",
    "$$\\ln\\mathcal{L}(\\boldsymbol{v}_r, \\theta) = - \\frac12 \\left[ (\\boldsymbol{v}_r - \\boldsymbol{\\mu})^T \\cdot \\boldsymbol{\\Sigma}^{-1} \\cdot (\\boldsymbol{v}_r - \\boldsymbol{\\mu}) + \\ln(|\\boldsymbol{\\Sigma}|) + N_\\mathrm{obs}\\ln(2\\pi) \\right]$$\n",
    "\n",
    "where the mean is the radial velocity function, $\\boldsymbol{\\mu} = f_r(\\boldsymbol{t}, \\theta)$ and the covariance is the squared exponential plus some white noise, $\\boldsymbol{\\Sigma} = \\boldsymbol{k}(t, t, \\theta) + \\sigma_v \\mathcal{I}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(params):\n",
    "    \"\"\"TODO: The prior now extends to GP kernel parameters sigma (s) and length (l).\"\"\"\n",
    "    a, p, s, l = params  # amplitude, period, sigma, length\n",
    "    if ... < a < ... and ... < p < ... and \\\n",
    "        ... < s < ... and ... < l < ...:\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "\n",
    "def log_likelihood(params, t, vr, vr_err):\n",
    "    \"\"\"TODO: GP log likelihood.\n",
    "    \n",
    "    Hints:\n",
    "        1. Add jitter to the covariance.\n",
    "        2. Use np.linalg.det() to get the determinant of cov (|cov|)\n",
    "    \"\"\"\n",
    "    a, p, s, l = params  # amplitude, period, sigma, length\n",
    "    \n",
    "    mu = ...\n",
    "    cov = ... \n",
    "    num_obs = vr.shape[0]  # This is N_obs\n",
    "    \n",
    "    # TODO: fill in the terms of the log_likelihood\n",
    "    return - 0.5 * (\n",
    "        ... + \\\n",
    "        ... + \\\n",
    "        ...\n",
    "    )\n",
    "\n",
    "\n",
    "def log_prob(params, t, vr, vr_err):\n",
    "    \"\"\"Posterior log probability.\"\"\"\n",
    "    lprior = log_prior(params)\n",
    "    if not np.isfinite(lprior):\n",
    "        return -np.inf\n",
    "    return lprior + log_likelihood(params, t, vr, vr_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup sampler, this time with 4 dimensions for the 4 model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dim, num_walkers = 4, 100\n",
    "sampler = emcee.EnsembleSampler(num_walkers, num_dim, log_prob, args=(t, vr, vr_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose our initial parameters. For the GP kernel, this may be the expected amplitude and length-scale of the correlated noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = [..., ..., ..., ...]  # TODO: fill in initial parameters\n",
    "shape = (num_walkers, num_dim)\n",
    "init_params = np.broadcast_to(init_params, shape) + 1e-6 * rng.normal(size=shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run MCMC. This may be slow; our `log_likelihood` is computationally expensive because of inverting the covariance matrix. There are ways we can improve this by reparameterising the likelihood (see e.g. Cholesky decompositions), but most probabilistic programming packages do this automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running burn-in...\")\n",
    "\n",
    "state, lp, _ = sampler.run_mcmc(init_params, 1000)\n",
    "\n",
    "print(\"Running production...\")\n",
    "sampler.reset()\n",
    "sampler.run_mcmc(state, 1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corner plot shows significant improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sampler.flatchain\n",
    "labels = ['amplitude', 'period', 'sigma', 'length']\n",
    "truths = list(truth.values()) + [None, None]\n",
    "corner(samples, show_titles=True, labels=labels, truths=truths);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions\n",
    "\n",
    "We can check how well our model recovers the truth by making predictions. The `predict` function only works for a given mean and kernel function, but we have posterior samples for many forms of these funtions.\n",
    "\n",
    "A GP is just a distribution over functions. From the posterior samples, we can get samples for the mean and kernel functions and combine them into the variable `gp`. `np.apply_along_axis` is a quick way to apply a function along an axis of an array. For samples from the posterior for `theta`, we obtain arrays of functions for the `mean` and `kernel` of the GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis = 1\n",
    "means = np.apply_along_axis(lambda theta: lambda t: radial_velocity(t, amplitude=theta[0], period=theta[1]), \n",
    "                            axis, samples)\n",
    "kernels = np.apply_along_axis(lambda theta: lambda t, tp: squared_exponential(t, tp, sigma=theta[2], length=theta[3]), \n",
    "                            axis, samples)\n",
    "\n",
    "gp = np.stack([means, kernels], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions for $v_r$, we sample from the multivariate normal governed by the predicted mean and covariance. This helper function does that for a given mean and kernel function, using our `predict` function from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(mean, kernel):\n",
    "    \"\"\"Samples the MV normal from the predicted mean and covariance.\"\"\"\n",
    "    mu_pred, cov_pred = predict(t_pred, t, vr, y_err=vr_err, mean=mean, kernel=kernel)\n",
    "    return rng.multivariate_normal(mu_pred, cov_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a subsample from the posterior GP, because the following step can be slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subsamples = 1000\n",
    "gp_subsample = rng.choice(gp, size=num_subsamples, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the subsamples of the GP, we get predictions for the radial velocity by again using `np.apply_along_axis`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_pred = np.apply_along_axis(lambda gp: get_predictions(gp[0], gp[1]), axis, gp_subsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(t, vr, yerr=vr_err, fmt='o', label='obs')\n",
    "\n",
    "vr_mean = vr_pred.mean(axis=0)\n",
    "vr_sd = vr_pred.std(axis=0)\n",
    "plt.plot(t_pred, vr_mean, label=r'$\\mu$')\n",
    "plt.fill_between(t_pred, vr_mean-vr_sd, vr_mean+vr_sd, alpha=0.33, color='C1', label=r'$2\\sigma$')\n",
    "\n",
    "plt.plot(t_pred, vr_true, linestyle='--', label='truth');\n",
    "plt.legend()\n",
    "plt.xlabel('time (days)')\n",
    "plt.ylabel('radial velocity');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 (Base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
